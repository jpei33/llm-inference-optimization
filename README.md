# llm-inference-optimization
Tinker Project on LLM inference optimization using speculative decoding and KV caching

